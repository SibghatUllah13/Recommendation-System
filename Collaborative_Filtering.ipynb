{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "def read_Books(file_name):\n",
    "    '''Read the Initial Book Data'''\n",
    "    cwd=os.getcwd()\n",
    "    path=cwd+\"\\\\\"+file_name\n",
    "    data_frame=pd.read_csv(path,sep=';',encoding='utf-8',error_bad_lines=False)\n",
    "    return data_frame\n",
    "\n",
    "def read_ratings(file_name):\n",
    "    '''Read the Initial Rating Table Given to use\n",
    "    as Part of input to Recommender System'''\n",
    "    cwd=os.getcwd()\n",
    "    path=cwd+\"\\\\\"+file_name\n",
    "    data_frame=pd.read_csv(path,sep=';',encoding='utf-8',header=0)\n",
    "    return data_frame\n",
    "\n",
    "def filter_items(df,min2):\n",
    "    '''Filter the Data Based on the number of rated items\n",
    "    to reduce complexity'''\n",
    "    multi = df.set_index(['Book','User'])\n",
    "    counts = df.groupby('Book').count()\n",
    "    ind = counts[(counts.User>min2)].index\n",
    "    l = []\n",
    "    ll = []\n",
    "    for i in ind:\n",
    "        data = multi.loc[i]\n",
    "        data = data.assign(Book = np.empty(data.shape[0]))\n",
    "        data.Book = i\n",
    "        l.append(data)\n",
    "    DF = l[0]\n",
    "    for j in range(1,len(l)):\n",
    "        DF = DF.append(l[j])\n",
    "    DF.reset_index(inplace=True)\n",
    "    return DF\n",
    "\n",
    "def filter_data(ratings,min1 = 200,min2 = 50):\n",
    "    '''Filter the Data Based on the number of rated items\n",
    "    to reduce complexity'''\n",
    "    multi = ratings.set_index(['User','Book'])\n",
    "    counts = ratings.groupby('User').count()\n",
    "    ind = counts[(counts.Book>min1)].index\n",
    "    l = []\n",
    "    ll = []\n",
    "    for i in ind:\n",
    "        data = multi.loc[i]\n",
    "        data = data.assign(User = np.empty(data.shape[0]))\n",
    "        data.User = i\n",
    "        l.append(data)\n",
    "    DF = l[0]\n",
    "    for j in range(1,len(l)):\n",
    "        DF = DF.append(l[j])\n",
    "    DF.reset_index(inplace=True)\n",
    "    DF = filter_items(DF,min2)\n",
    "    return DF\n",
    "\n",
    "def find_samples(Data):\n",
    "    '''Make Training Set & Test Set from Given Data'''\n",
    "    Train_indices = []\n",
    "    Test_indices = []\n",
    "    counter = 0\n",
    "    c = 0\n",
    "    rows = (Data.notnull().sum(axis=1).sort_values(ascending = False)>1)[:Data.shape[0]//5].index\n",
    "#     cols = (Data.notnull().sum().sort_values(ascending = False)>1).index[:20]\n",
    "    while c != 5:\n",
    "#         rows = np.random.choice(Data.shape[0],Data.shape[0]//5, replace=False)\n",
    "        cols = np.random.choice(Data.shape[1],Data.shape[1]//5, replace=False)\n",
    "        Test = Data.ix[rows,cols]\n",
    "        Data_Copy = Data.copy()\n",
    "        Data_Copy.at[Test.index,Test.columns] = np.nan\n",
    "        Train = Data_Copy\n",
    "        if (Train.notnull().sum(axis = 1) ==0 ).sum() == 0:\n",
    "#             if Data.iloc[rows,cols].count().sum() > 10:\n",
    "            Test_indices.append(Data.loc[Test.index,Test.columns])\n",
    "            Train_indices.append(Train)\n",
    "            c += 1\n",
    "            #print('c = {}'.format(c))       \n",
    "        counter += 1\n",
    "        #print(counter, end = ',')\n",
    "    return Train_indices, Test_indices\n",
    "\n",
    "def take_input(file_name):\n",
    "    '''Read From the File, ISBN Per Line (For Online Version only)'''\n",
    "    isbn = []\n",
    "    rat = []\n",
    "    target = open(file_name,mode='r',encoding='utf8')\n",
    "    lines = target.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1]\n",
    "        isbn.append (str(line[:-2]))\n",
    "        rat.append (int(line[-1]))\n",
    "    return isbn,rat\n",
    "\n",
    "def baseline(bd):\n",
    "    '''Create the Baseline estimators for\n",
    "    each User and item to make better predictions'''\n",
    "    mean= np.mean(bd._get_numeric_data().mean())\n",
    "    col_wise_mean = np.array(bd.mean(skipna=True,axis=0))\n",
    "    row_wise_mean = np.array(bd.mean(skipna=True,axis=1))\n",
    "    bd_df= pd.DataFrame(np.zeros(shape=(bd.shape[0],bd.shape[1])))\n",
    "    bd_df.columns=bd.columns\n",
    "    bd_df.index = bd.index\n",
    "    for i in range(len(row_wise_mean)):\n",
    "        for j in range(len(col_wise_mean)):\n",
    "            bd_df.iloc[i,j] = mean+(row_wise_mean[i]-mean)+(col_wise_mean[j]-mean)\n",
    "    return bd_df\n",
    "\n",
    "def convert_standard_normal(data):\n",
    "    ''' Subtract Each row from its Mean'''\n",
    "    return data.sub(data.mean(axis=1), axis=0)\n",
    "\n",
    "def similar_items(nulldata,df,user,item):\n",
    "    ''' Find the Most Similar Items wrt the Item of Interest\n",
    "    alongside their cosine similarity score'''\n",
    "    related_items =  nulldata.loc[:,str(user)] [nulldata.loc[:,str(user)].notnull()].index\n",
    "    item_of_interest = df.ix[item]\n",
    "    item_of_interest = pd.DataFrame([item_of_interest],[item])\n",
    "    other_items = df.ix[related_items]\n",
    "    cosine = cosine_similarity(item_of_interest,other_items).ravel()\n",
    "    ranking = cosine.argsort()[-3:][::-1]\n",
    "    if len(ranking)>2:\n",
    "        most_sim_item = [related_items[ranking[0]],related_items[ranking[1]],related_items[ranking[2]]]\n",
    "        respective_score = [cosine[ranking[0]],cosine[ranking[1]],cosine[ranking[2]]]\n",
    "    else:\n",
    "        if len(ranking)>1:\n",
    "            most_sim_item = [related_items[ranking[0]],related_items[ranking[1]]]\n",
    "            respective_score = [cosine[ranking[0]],cosine[ranking[1]]]\n",
    "        else:\n",
    "            most_sim_item = [related_items[ranking[0]]]\n",
    "            respective_score = [cosine[ranking[0]]]\n",
    "    return most_sim_item, respective_score\n",
    "\n",
    "def predict(org_data,baseline_data,data,user,item):\n",
    "    '''Predict the rating using the formula'''\n",
    "    universal_rating = np.nanmean(org_data.values)\n",
    "    related_items =  org_data.loc[:,str(user)] [org_data.loc[:,str(user)].notnull()].index\n",
    "    if len(related_items)>1:\n",
    "        neighbour, sij = similar_items(org_data,data,user,item)\n",
    "        r_xj = np.zeros(len(neighbour))\n",
    "        b_xj = np.zeros(len(neighbour))\n",
    "        counter=0\n",
    "        for item in neighbour:\n",
    "            b_xj[counter] = baseline_data.loc[item,user]\n",
    "            counter+=1\n",
    "        for i in range(len(neighbour)):\n",
    "            r_xj[i] =(org_data.loc[neighbour[i],str(user)])\n",
    "        to=0\n",
    "        for i in range(len(neighbour)):\n",
    "            to+= sij[i]*((r_xj[i])-(b_xj[i]))\n",
    "        return baseline_data.loc[item,user]+(to/sum(sij))\n",
    "    else:\n",
    "        return universal_rating\n",
    "    \n",
    "def normalize_ratings(filled_data):\n",
    "    return ((filled_data-np.min(filled_data.values))/(np.max(filled_data.values)-np.min(filled_data.values)))*10\n",
    "\n",
    "def cal_error(key):\n",
    "    '''Calculate Error for a Specific Train Set'''\n",
    "    train = pd.read_csv('Filled_Matrix'+str(key)+'.csv',encoding='utf8')\n",
    "    train.set_index('Book',inplace=True)\n",
    "    test = pickle.load(open('Test'+str(key)+'.p','rb'))\n",
    "    nr = normalize_ratings(train)\n",
    "    return (np.nansum(abs(nr.loc[test.index,test.columns]-test).values))/test.notnull().sum().sum()/10\n",
    "\n",
    "def online_pred(null_data,std_normal_data,item):\n",
    "    neb,sij = similar_items(null_data,std_normal_data,'883',item)\n",
    "    bxi = baseline_data.loc[item,'883']\n",
    "    bxj = np.zeros(shape=len(neb))\n",
    "    rxj = np.zeros(shape=len(neb))\n",
    "    for i in range(len(neb)):\n",
    "        bxj [i] = baseline_data.loc[neb[0],'883']\n",
    "        rxj [i] = std_normal_data.loc[neb[0],'883']\n",
    "    tot = 0\n",
    "    for i in range(len(neb)):\n",
    "        tot+= (sij[i]*(rxj[i]-bxj[i]))/sij[i]\n",
    "    return tot+bxi\n",
    "\n",
    "def book_data():\n",
    "    '''Save the Book Data to print Results'''\n",
    "    Books_data=read_Books('BX-Books.csv')\n",
    "    books =Books_data.iloc[:,0:3]\n",
    "    books.columns = ['ISBN','Title','Author']\n",
    "    books.set_index('ISBN',inplace=True)\n",
    "    books.to_csv('Books.csv',encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Version Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Data has been Created\n"
     ]
    }
   ],
   "source": [
    "'''Read the Initial Data and Reduce it greatly'''\n",
    "Ratings= read_ratings('BX-Book-Ratings.csv')\n",
    "Ratings.columns=['User','Book','Rating']\n",
    "new_data = filter_data(Ratings)\n",
    "pivot_data = new_data.pivot(index='Book',columns='User',values='Rating')\n",
    "pivot_data.to_csv('pivot.csv',encoding='utf8')\n",
    "print ('New Data has been Created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train0.p and Test0.p have been created!\n",
      "Train1.p and Test1.p have been created!\n",
      "Train2.p and Test2.p have been created!\n",
      "Train3.p and Test3.p have been created!\n",
      "Train4.p and Test4.p have been created!\n"
     ]
    }
   ],
   "source": [
    "'''Generate 5 Training & Test Datasets'''\n",
    "data = pd.read_csv('pivot.csv',encoding='utf8')\n",
    "data.set_index('Book',inplace=True)\n",
    "Trains,Tests = find_samples(data.T)\n",
    "for i in range(5):\n",
    "    pickle.dump(Trains[i].T,open('Train{}.p'.format(i),'wb'))\n",
    "    pickle.dump(Tests[i].T,open('Test{}.p'.format(i),'wb'))\n",
    "    print('Train{}.p and Test{}.p have been created!'.format(i,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Estimators have been Created\n"
     ]
    }
   ],
   "source": [
    "'''Calculate the baseline estimator DF'''\n",
    "bd_data = baseline(data)\n",
    "bd_data.to_csv('Baseline.csv',encoding ='utf8')\n",
    "print ('Baseline Estimators have been Created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Get a Specific Train Set and Estimate Ratings'''\n",
    "data = pickle.load(open('Train4.p','rb'))\n",
    "std_normal_data = convert_standard_normal(data)\n",
    "org_data = data\n",
    "std_normal_data.fillna(0,inplace=True)\n",
    "filled_data = std_normal_data\n",
    "baselin_data = pd.read_csv('Baseline.csv',encoding='utf8')\n",
    "baselin_data.set_index('Book',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions to be filled have been found\n"
     ]
    }
   ],
   "source": [
    "'''Find out the Positions of NAN values'''\n",
    "pos = list(zip(*data.as_matrix().nonzero()))\n",
    "array= []\n",
    "for c_pos in pos:\n",
    "    row_id = data.iloc[c_pos[0],:].name\n",
    "    col_id = data.iloc[:,c_pos[1]].name\n",
    "    array.append([row_id,col_id])\n",
    "print ('Positions to be filled have been found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Fill the Matrix with Estimated Predictions'''\n",
    "for position in array:\n",
    "    data.loc[position[0],position[1]] = predict(org_data,bd_data,std_normal_data,position[1],position[0])\n",
    "data.to_csv('Filled_Matrix4.csv',encoding='utf8')\n",
    "print ('Matrix4 has been Filled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error is :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21861583315257108"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Calculate MAE for all the training sets'''\n",
    "MAE = np.ones(5)\n",
    "for key in range(5):\n",
    "    MAE[key] = (cal_error(key))\n",
    "print ('Mean Absolute Error is :') \n",
    "np.mean(MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Version Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Processing Done\n"
     ]
    }
   ],
   "source": [
    "'''Some Pre Processing for Online Version'''\n",
    "f0 = pd.read_csv('Filled_Matrix0.csv',encoding='utf8')\n",
    "f1 = pd.read_csv('Filled_Matrix1.csv',encoding='utf8')\n",
    "f2 = pd.read_csv('Filled_Matrix2.csv',encoding='utf8')\n",
    "f3 = pd.read_csv('Filled_Matrix3.csv',encoding='utf8')\n",
    "f4 = pd.read_csv('Filled_Matrix4.csv',encoding='utf8')\n",
    "f0.set_index('Book',inplace=True)\n",
    "f1.set_index('Book',inplace=True)\n",
    "f2.set_index('Book',inplace=True)\n",
    "f3.set_index('Book',inplace=True)\n",
    "f4.set_index('Book',inplace=True)\n",
    "f0 = f0.add(f1)\n",
    "f0 = f0.add(f2)\n",
    "f0 = f0.add(f3)\n",
    "f0 = f0.add(f4)\n",
    "f0 = f0/5\n",
    "print ('Pre Processing Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You Belong To Me Written by Mary Higgins Clark\n",
      "2nd Chance Written by James Patterson\n",
      "The Murder Book Written by Jonathan Kellerman\n"
     ]
    }
   ],
   "source": [
    "books = pd.read_csv('Books.csv',encoding='utf8')\n",
    "books.set_index('ISBN',inplace=True)\n",
    "new_user = pd.DataFrame(np.zeros(f0.iloc[:,0].shape))\n",
    "new_user.index = f0.index\n",
    "new_user.columns =['883']\n",
    "new_user.iloc[:,:] = np.nan\n",
    "'''Fill the Input File Randomly'''\n",
    "random_indices = random.sample(range(504), 70)\n",
    "indices = new_user.iloc[random_indices,:].index\n",
    "values = np.random.randint(10, size=(len(indices)))\n",
    "target = open('input.txt',mode='w',encoding='utf8')\n",
    "for i in range(len(indices)):\n",
    "    target.write(indices[i]+' '+str(values[i]))\n",
    "    target.write('\\n')\n",
    "target.close()\n",
    "#print ('Input File is ready to Use')\n",
    "isbn, rat = take_input('input.txt')\n",
    "for i in range(len(isbn)):\n",
    "    new_user.loc[isbn[i],:] = rat[i]\n",
    "f0['883'] = new_user.iloc[:,0]\n",
    "'''Some More Pre Processing'''\n",
    "baseline_data = baseline(f0)\n",
    "null_data = f0\n",
    "std_normal_data = convert_standard_normal(f0)\n",
    "std_normal_data.fillna(0,inplace=True)\n",
    "filled_data = f0\n",
    "'''Find Out NAN'''\n",
    "nan_indices = []\n",
    "for i in range(f0.shape[0]):\n",
    "    if f0.iloc[i,-1]!=f0.iloc[i,-1]:\n",
    "        nan_indices.append(str(f0.iloc[i,:].name))\n",
    "for index in nan_indices:\n",
    "    filled_data.loc[index,'883'] = online_pred(null_data,std_normal_data,index)\n",
    "estimated = filled_data.loc[nan_indices,'883'].values\n",
    "sorting = estimated.argsort()[-3:][::-1]\n",
    "print (books.loc[str(nan_indices[sorting[0]]),'Title']+' Written by '+books.loc[str(nan_indices[sorting[0]]),'Author'])\n",
    "print (books.loc[str(nan_indices[sorting[1]]),'Title']+' Written by '+books.loc[str(nan_indices[sorting[1]]),'Author'])\n",
    "print (books.loc[str(nan_indices[sorting[2]]),'Title']+' Written by '+books.loc[str(nan_indices[sorting[2]]),'Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
