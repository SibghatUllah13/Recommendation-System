{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def convert_lower(sentence):\n",
    "    '''Lowercases the input'''\n",
    "    return str.lower(sentence)\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    '''Remove punctuation from the input'''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    l=tokenizer.tokenize(sentence)\n",
    "    return (\" \".join(l))\n",
    "\n",
    "def tokenize(sentence):\n",
    "    '''Tokenizes the input'''\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def remove_stop_words(word_list):\n",
    "    '''Removes Stop Words'''\n",
    "    array=[]\n",
    "    for word in word_list:\n",
    "        if word not in stopwords.words('english'):\n",
    "            array.append(word)\n",
    "    return array\n",
    "\n",
    "def clean_single_Item(Single_recipe):\n",
    "    '''Uses Natural Language Processing for cleaning the input'''\n",
    "    new_recipe=[]\n",
    "    stemmer = PorterStemmer()\n",
    "    for recipe_element in Single_recipe:\n",
    "        temp=recipe_element\n",
    "        temp=convert_lower(temp)\n",
    "        temp=remove_punctuation(temp)\n",
    "        temp=tokenize(temp)\n",
    "        temp=remove_stop_words(temp)\n",
    "        final_words=[]\n",
    "        for word in temp:\n",
    "            final_words.append(stemmer.stem(word))\n",
    "        final_words=\" \".join(final_words)\n",
    "        new_recipe.append(final_words)\n",
    "    return new_recipe\n",
    "\n",
    "def write_to_file(Recipe,file_name):\n",
    "    '''Writes on the HD the document'''\n",
    "    target=open(file_name,'a',encoding='utf-8')\n",
    "    for element in Recipe:\n",
    "        target.write(str(element))\n",
    "        target.write('\\n')\n",
    "    target.close()\n",
    "    \n",
    "def clean_all_Item(recipe_file,output_file):\n",
    "    '''Uses subfunctions to clean all the items for TFIDF'''\n",
    "    target=open(recipe_file,'r',encoding='utf-8')\n",
    "    counter=0\n",
    "    Single_Recipe=[]\n",
    "    for ind_element in target:\n",
    "        if ind_element==\"\\n\":\n",
    "            Single_Recipe.append(\"No Information\")\n",
    "        else:\n",
    "            Single_Recipe.append(ind_element)\n",
    "        counter+=1\n",
    "        if counter>=3:\n",
    "            nice_recipe=clean_single_Item(Single_Recipe)\n",
    "            write_to_file(nice_recipe,output_file)\n",
    "            counter=0\n",
    "            Single_Recipe[:]=[]\n",
    "    target.close()\n",
    "\n",
    "def read_Users(file_name):\n",
    "    '''Loads Users Dataset'''\n",
    "    cwd=os.getcwd()\n",
    "    path=cwd+\"\\\\\"+file_name\n",
    "    data_frame=pd.read_csv(path,sep=';',encoding='utf-8',header=0)\n",
    "    return data_frame\n",
    "\n",
    "def read_Books(file_name):\n",
    "    '''Loads Books Dataset '''\n",
    "    cwd=os.getcwd()\n",
    "    path=cwd+\"\\\\\"+file_name\n",
    "    data_frame=pd.read_csv(path,sep=';',encoding='utf-8',error_bad_lines=False)\n",
    "    return data_frame\n",
    "\n",
    "def read_ratings(file_name):\n",
    "    '''Loads Ratings Dataset'''\n",
    "    cwd=os.getcwd()\n",
    "    path=cwd+\"\\\\\"+file_name\n",
    "    data_frame=pd.read_csv(path,sep=';',encoding='utf-8',header=0)\n",
    "    return data_frame\n",
    "\n",
    "def run():\n",
    "    '''Loads all the initial data'''\n",
    "    User_data=read_Users('BX-Users.csv')\n",
    "    User_data.columns = ['User-ID','Location','Age']\n",
    "    Books_data=read_Books('BX-Books.csv')\n",
    "    Ratings= read_ratings('BX-Book-Ratings.csv')\n",
    "    Ratings.columns = ['User','Book','Rating']\n",
    "    return User_data,Books_data,Ratings\n",
    "\n",
    "def clean_data():\n",
    "    '''Loads and organizes the 3 initial datasets'''\n",
    "    User,Book,Ratings=run()\n",
    "    Book=Book.ix[:,0:5]\n",
    "    Book=Book.dropna()\n",
    "    Book.columns = ['ISBN','Title','Author','Year','Publisher']\n",
    "    User.set_index('User-ID',inplace = True)\n",
    "    Book.set_index('ISBN',inplace=True)\n",
    "    return User,Book,Ratings\n",
    "\n",
    "def get_items_of_interest(Train, books):\n",
    "    '''Takes Train set and books dataframe and returns\n",
    "    itemset without NaNs inside it ready for creating Item Profiles'''\n",
    "    books = books.loc[Train.columns,:].dropna()  # Remove NaNs\n",
    "    mask = (books.Year == '0')\n",
    "    mask2 = (books.Year == 0)\n",
    "    books.Year[mask] = round(np.mean(books.Year.values.astype('int'))) # Interpolates missing years\n",
    "    books.Year[mask2] = round(np.mean(books.Year.values.astype('int'))) # Interpolates missing years\n",
    "    return books\n",
    "\n",
    "def write_Items(file_name, Items):\n",
    "    '''Writes documents to the HD. Each written on 3 new lines'''\n",
    "    f = open(file_name, 'w', encoding='utf8')\n",
    "    for i in range(len(Items)):\n",
    "        f.write(str(Items.Title[i])+'\\n')\n",
    "        f.write(str(Items.Author[i])+'\\n')\n",
    "        f.write(str(Items.Publisher[i])+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def read_3line_items(file_name):\n",
    "    '''Takes a file name where the text file contains each document distributed on 3 new lines each and gives\n",
    "    a list containing strings of documents as elements as output'''\n",
    "    l = []\n",
    "    f = open(file_name, 'r', encoding='utf8')\n",
    "    lines = f.readlines()\n",
    "    f.close\n",
    "    for i in range(len(lines)//3):\n",
    "        l.append(str(lines[i*3])[:-1]+' ' + str(lines[i*3+1])[:-1]+' '+str(lines[i*3+2])[:-1])\n",
    "    return l\n",
    "\n",
    "def create_tf_idf(file_name):\n",
    "    '''Takes a list as input in which each element is a string representing each document and\n",
    "    gives tfidf np.array as outpput'''\n",
    "    corpus = read_3line_items(file_name)\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    tfidf = transformer.fit_transform(X.toarray())\n",
    "    return tfidf.toarray()\n",
    "\n",
    "def rated_items(filtered, shape, user_ids):\n",
    "    '''Takes utility matrix and returns users and the corresponding books he/she has read(ISBN)'''\n",
    "    user_item = pd.DataFrame(np.zeros(shape=shape))\n",
    "    for i in range(filtered.shape[1]):\n",
    "        if len((filtered.iloc[:,i].dropna().index))>1:\n",
    "            user_item.iloc[i,:len(filtered.iloc[:,i].dropna().index)]= filtered.iloc[:,i].dropna().index.values\n",
    "        else:\n",
    "            user_item.iloc[i,:len(filtered.iloc[:,i].dropna().index)]= str(filtered.iloc[:,i].dropna().index.values)[2:-2]\n",
    "    user_item.index = user_ids\n",
    "    return user_item\n",
    "\n",
    "def Year(Read_Books,Items):\n",
    "    '''Gives the matrix which contains users and corresponding item's publication years '''\n",
    "    Years = pd.DataFrame(np.zeros(Read_Books.shape), index = Read_Books.index, columns=Read_Books.columns)\n",
    "    for i in range(Years.shape[0]):\n",
    "        for j in range((Read_Books.iloc[i] != 0).sum()):\n",
    "            if Read_Books.iloc[i,j] != 0:\n",
    "                Years.iloc[i,j] = Items[Read_Books.iloc[i,j]]\n",
    "    return Years\n",
    "\n",
    "def AVGYear(Rec_System,Read_Books,Items):\n",
    "    '''Gives average year for each user'''\n",
    "    Years = Year(Read_Books, Items)\n",
    "    Yearavg = pd.DataFrame(np.zeros((Rec_System.shape[0],1)),index = Rec_System.index, columns=['AVGYear'])\n",
    "    for i in range(Yearavg.shape[0]):\n",
    "        Yearavg.iloc[i,0] = np.mean(Years.iloc[i][Years.iloc[i]>0])\n",
    "    return Yearavg\n",
    "\n",
    "def Diff_Matrix(Rec_System,Read_Books,Items):\n",
    "    '''Gives User by Item matrix and values are difference of years between average of\n",
    "    year of publication of read books and all books'''\n",
    "    AVGYears = AVGYear(Rec_System,Read_Books,Items)\n",
    "    YearDiff = pd.DataFrame(np.zeros((Read_Books.shape[0],len(Items))),index = Read_Books.index, columns=Items.index)\n",
    "    for i in range(YearDiff.shape[0]):\n",
    "        YearDiff.iloc[i] = abs((AVGYears.iloc[i,0] - Items).values)\n",
    "    YearDiff=(YearDiff-YearDiff.min().min())/(YearDiff.max().max()-YearDiff.min().min())\n",
    "    return YearDiff\n",
    "\n",
    "def ISBN_REC_SYS(Rec_System,Read_Books,YearDiff,itz,alfa):\n",
    "    '''Recommendations for each user. Values are ISBNs (actually\n",
    "    values are numbers which can be translated to ISBNs with specific table)'''\n",
    "    l = np.zeros(Rec_System.shape[1])\n",
    "    for i in range(Rec_System.shape[0]):\n",
    "        leng = np.array((Rec_System.iloc[i,:].drop(Read_Books.iloc[i,:][Read_Books.iloc[i,:] != 0].values))).shape[0]\n",
    "        empty = np.ones(Rec_System.shape[1],dtype='int32')*-1\n",
    "        ISBNS = np.array((Rec_System.iloc[i,:]+alfa*YearDiff.iloc[i,:]).sort_values(ascending = False).\n",
    "                 drop(Read_Books.iloc[i,:][Read_Books.iloc[i,:] != 0].values).index)\n",
    "        empty[:leng] = itz.loc[ISBNS].values.flatten()\n",
    "        l = np.vstack((l,empty))\n",
    "    return l[1:,:]\n",
    "\n",
    "def RATINGS_REC_SYS(Rec_System,Read_Books,YearDiff,alfa):\n",
    "    '''Recommendations for each user. Values are the predicted ratings'''\n",
    "    l = np.zeros(Rec_System.shape[1])\n",
    "    for i in range(Rec_System.shape[0]):\n",
    "        leng = np.array((Rec_System.iloc[i,:].drop(Read_Books.iloc[i,:][Read_Books.iloc[i,:] != 0].values))).shape[0]\n",
    "        empty = np.ones(Rec_System.shape[1])*-1\n",
    "        empty[:leng] = np.array((Rec_System.iloc[i,:]+alfa*YearDiff.iloc[i,:]).sort_values(ascending = False).\n",
    "                 drop(Read_Books.iloc[i,:][Read_Books.iloc[i,:] != 0].values).values)\n",
    "        l = np.vstack((l,empty))\n",
    "    return l[1:,:]\n",
    "\n",
    "def find_samples(Data):\n",
    "    '''Searches for 5 train and test sets so that when the test set is held out from the train set,\n",
    "    train sets contains at least one non-NaN value for each user'''\n",
    "    Train_indices = []\n",
    "    Test_indices = []\n",
    "    counter = 0\n",
    "    c = 0\n",
    "    rows = (Data.notnull().sum(axis=1).sort_values(ascending = False)>1)[:Data.shape[0]//5].index\n",
    "    while c != 5:\n",
    "        cols = np.random.choice(Data.shape[1],Data.shape[1]//5, replace=False)\n",
    "        Test = Data.ix[rows,cols]\n",
    "        Data_Copy = Data.copy()\n",
    "        Data_Copy.at[Test.index,Test.columns] = np.nan\n",
    "        Train = Data_Copy\n",
    "        if (Train.notnull().sum(axis = 1) == 0).sum() == 0:\n",
    "            Test_indices.append(Data.loc[Test.index,Test.columns])\n",
    "            Train_indices.append(Train)\n",
    "            c += 1\n",
    "            print('c = {}'.format(c))       \n",
    "        counter += 1\n",
    "        print(counter, end = ',')\n",
    "    return Train_indices, Test_indices\n",
    "\n",
    "def filter_items(df,min2):\n",
    "    '''Filters data so that each book is rated at least by min2 users'''\n",
    "    multi = df.set_index(['Book','User'])\n",
    "    counts = df.groupby('Book').count()\n",
    "    ind = counts[(counts.User>min2)].index\n",
    "    l = []\n",
    "    ll = []\n",
    "    for i in ind:\n",
    "        data = multi.loc[i]\n",
    "        data = data.assign(Book = np.empty(data.shape[0]))\n",
    "        data.Book = i\n",
    "        l.append(data)\n",
    "    DF = l[0]\n",
    "    for j in range(1,len(l)):\n",
    "        DF = DF.append(l[j])\n",
    "    DF.reset_index(inplace=True)\n",
    "    return DF\n",
    "\n",
    "def filter_data(ratings,min1 = 70,min2 = 10):\n",
    "    '''Filters data so that each user has rated at least min1 book and each book is rated at least by min2 users'''\n",
    "    multi = ratings.set_index(['User','Book'])\n",
    "    counts = ratings.groupby('User').count()\n",
    "    ind = counts[(counts.Book>min1)].index\n",
    "    l = []\n",
    "    ll = []\n",
    "    for i in ind:\n",
    "        data = multi.loc[i]\n",
    "        data = data.assign(User = np.empty(data.shape[0]))\n",
    "        data.User = i\n",
    "        l.append(data)\n",
    "    DF = l[0]\n",
    "    for j in range(1,len(l)):\n",
    "        DF = DF.append(l[j])\n",
    "    DF.reset_index(inplace=True)\n",
    "    DF = filter_items(DF,min2)\n",
    "    return DF\n",
    "\n",
    "def MAE(filename1,filename2,filename3,filename4):\n",
    "    '''Takes pickle file names and gives Mean Absolute Error'''\n",
    "    '''Test.p file, Scores.p file, ISBNS.p file, Items.p file'''\n",
    "    Test = pickle.load(open(filename1,'rb'))\n",
    "    RATINGS_RECOMMENDATIONS = pickle.load(open(filename2,'rb'))\n",
    "    ISBN_RECOMMENDATIONS = pickle.load(open(filename3,'rb'))\n",
    "    itz = pickle.load(open(filename4,'rb'))\n",
    "    Test.dropna(axis = 0, how = 'all', inplace = True)\n",
    "    MAE = 0\n",
    "    for i in range(Test.shape[0]):\n",
    "        user = Test.iloc[i].name  #User id\n",
    "        keys = ISBN_RECOMMENDATIONS.loc[user].values  # ISBNS for that user\n",
    "        keys = keys[keys!=-1]     # ISBN keys which was not read by that user\n",
    "        keys = np.array(keys[keys!=-1],dtype='int32')   # Array of ISBN keys which was not read by that user\n",
    "        predicted_books = itz.reset_index().iloc[keys].ISBN.values  #Books not read (to be recommended)\n",
    "        scores = RATINGS_RECOMMENDATIONS.loc[user][:len(predicted_books)]\n",
    "        predicted_scores = pd.Series(scores.values, index = predicted_books)\n",
    "\n",
    "        mask = Test.iloc[i,:].notnull()\n",
    "        books_read, actual_ratings = Test.iloc[i][mask].index, Test.iloc[i][mask].values \n",
    "        \n",
    "        MAE += abs((predicted_scores[books_read].values - actual_ratings)).sum()\n",
    "    return MAE/Test.notnull().sum().sum()\n",
    "\n",
    "def modif_single_user(ratings_filtered,user_id, items):\n",
    "    '''Creates user profile for one user'''\n",
    "    ratings = ratings_filtered.loc[user_id].dropna()\n",
    "    centered =  ratings - ratings.mean() #centered ratings for the user\n",
    "    read = centered.index #books read by the user\n",
    "    call = items.loc[read].loc[:,items.loc[read].sum()>0].columns.astype('int32') #vector containing only positive sum column indices\n",
    "    user = np.zeros(items.shape[1])\n",
    "    user[call] = (items.loc[read].iloc[:,call].\n",
    "                  apply(lambda x: x*centered)/\n",
    "                  items.loc[read].\n",
    "                  iloc[:,call].\n",
    "                  sum()).sum()\n",
    "    return user\n",
    "    \n",
    "def user_profs(Ratings, filename, Items,alfa = 1):\n",
    "    '''Creates User Profile Matrix'''\n",
    "    items = item_profile(filename, Items,Ratings,alfa)\n",
    "    a = pd.DataFrame(np.zeros((Ratings.shape[1],items.shape[1])))\n",
    "    Rate = Ratings.T\n",
    "    for i in range(Rate.shape[0]):\n",
    "        a.iloc[i] = (modif_single_user(Rate, Ratings.columns[i], items))\n",
    "        print(i, end=',')\n",
    "    return a, items\n",
    "    \n",
    "def predictions(Train,filename, Items, user_ids, book_ids, alfa = 1):\n",
    "    '''Creates prediction matrix'''\n",
    "    User_Profiles,item_profiles = user_profs(Train, filename, Items, alfa) \n",
    "    Rec_System = pd.DataFrame(cosine_similarity(User_Profiles,item_profiles), index = user_ids, columns=book_ids)\n",
    "    return Rec_System, item_profiles\n",
    "\n",
    "def item_profile(filename, Items, Train, alfa=1):\n",
    "    '''Creates item profiles'''\n",
    "    tfidf = create_tf_idf(filename)  #Create TFIDF\n",
    "    item_profiles = pd.DataFrame(tfidf) #Dataframe format\n",
    "    item_profiles['ISBN'] = Items.index #Add ISBN column\n",
    "    item_profiles.set_index('ISBN',inplace=True) #ISBNs as index\n",
    "    avg_rating = np.zeros(Train.shape[0]) #Empty matrix\n",
    "    avg_all = np.nanmean(Train.values) #Mean Rating\n",
    "    for i in range(item_profiles.shape[0]):\n",
    "        book = item_profiles.index[i]\n",
    "        if len(Train.loc[book].dropna()) >0:\n",
    "            avg_rating[i] = Train.loc[book].mean()  #average rating of the book\n",
    "        else:\n",
    "            avg_rating[i] = avg_all #average rating of all the books\n",
    "    item_profiles['{}'.format(item_profiles.shape[1])] = alfa*avg_rating\n",
    "    return item_profiles\n",
    "\n",
    "def single_user(user):\n",
    "    '''User profile for online version'''\n",
    "    ratings = user.Rating\n",
    "    centered =  ratings - ratings.mean() #centered ratings for the user\n",
    "    read = centered.index #books read by the user\n",
    "    call = (items.loc[read].loc[:,items.loc[read].sum()>0].\n",
    "            columns.astype('int32')) #vector containing only positive sum column indices\n",
    "    user = np.zeros(items.shape[1]) #empty vector\n",
    "    user[call] = (items.loc[read].iloc[:,call]. #select only populatable columns\n",
    "                  apply(lambda x: x*centered)/  #apply function of weighted average\n",
    "                  items.loc[read].\n",
    "                  iloc[:,call].\n",
    "                  sum()).sum()\n",
    "    return user\n",
    "\n",
    "def Recommend(sim,Items,user,alfa = 50):\n",
    "    '''Recommendation output for online version user'''\n",
    "    readbooks = user.index #Books read by the user\n",
    "    AVGY = np.mean(Items.loc[readbooks]) #Average year for this user\n",
    "    Yeardiff = np.abs(Items - AVGY) #Compute year difference for each book\n",
    "    Yearsim = (1 - ((Yeardiff - Yeardiff.min())/(Yeardiff.max()-Yeardiff.min()))) #Standardize\n",
    "    Yearsim = Yearsim.drop(readbooks) #Only not read books\n",
    "    data = pd.Series(sim, index = Items.index).drop(readbooks) #Cosine similarities\n",
    "    data = data + (alfa*Yearsim) # Total similarity\n",
    "    data = (data - data.min())/(data.max()-data.min())*10  #Standardize\n",
    "    data.sort_values(0, ascending = False, inplace = True) \n",
    "    print('Recommended books: {}'.format(data[:5].index.values))\n",
    "    print('Predicted ratings: {}'.format(np.round(data[:5].values.flatten(),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6452: expected 8 fields, saw 9\\nSkipping line 43667: expected 8 fields, saw 10\\nSkipping line 51751: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 92038: expected 8 fields, saw 9\\nSkipping line 104319: expected 8 fields, saw 9\\nSkipping line 121768: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 144058: expected 8 fields, saw 9\\nSkipping line 150789: expected 8 fields, saw 9\\nSkipping line 157128: expected 8 fields, saw 9\\nSkipping line 180189: expected 8 fields, saw 9\\nSkipping line 185738: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 209388: expected 8 fields, saw 9\\nSkipping line 220626: expected 8 fields, saw 9\\nSkipping line 227933: expected 8 fields, saw 11\\nSkipping line 228957: expected 8 fields, saw 10\\nSkipping line 245933: expected 8 fields, saw 9\\nSkipping line 251296: expected 8 fields, saw 9\\nSkipping line 259941: expected 8 fields, saw 9\\nSkipping line 261529: expected 8 fields, saw 9\\n'\n",
      "C:\\Users\\conne\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#Loading Files\n",
    "users, books, ratings = clean_data()\n",
    "# ratings.set_index('User',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = filter_data(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.pivot('User','Book','Rating').to_csv('Ratings.csv',encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # # Load Data\n",
    "# Data = pd.read_csv('Ratings.csv',encoding='utf8')\n",
    "# Data.set_index('User',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Taking only those books in the table that are present in books dataframe.\n",
    "# #Also removing those rows that have only NaN values inside them\n",
    "# intersection = Data.columns.intersection(books.index)\n",
    "# Data = Data.loc[:,intersection]\n",
    "# Data = Data.loc[((Data.notnull() > 0).sum(axis = 1) > 0),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating Train and Test sets\n",
    "\n",
    "# Trains, Tests = find_samples(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # #Checking if Tests are not fully overlapping\n",
    "\n",
    "# for i in range(5):\n",
    "#     for j in range(i+1,5):\n",
    "#         print((sorted(set(Tests[i].columns))==sorted(set(Tests[j].columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Save Train and Test datasets on the HD\n",
    "# for i in range(5):\n",
    "#     pickle.dump(Trains[i],open('Train{}.p'.format(i),'wb'))\n",
    "#     pickle.dump(Tests[i],open('Test{}.p'.format(i),'wb'))\n",
    "#     print('Train{}.p and Test{}.p have been created!'.format(i,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the number of Train set0\n"
     ]
    }
   ],
   "source": [
    "# Load Train and Test Datasets\n",
    "Testing = int(input(prompt = 'Input the number of Train set'))\n",
    "t = time.clock()\n",
    "Train, Test = pickle.load(open('Train{}cb.p'.format(Testing),'rb')), pickle.load(open('Test{}cb.p'.format(Testing),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ratings = 0\n",
    "# Data = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Scores for each User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Items = get_items_of_interest(Train,books) #Only the books which are in the Train Set\n",
    "Items.Year = Items.Year.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Save Items to the HD\n",
    "# pickle.dump(Items.reset_index()['index'],open('Items.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Create corpus for TFIDF Transformer\n",
    "\n",
    "# write_Items('Dirty_Text.txt',Items)\n",
    "# clean_all_Item('Dirty_Text.txt','Clear_Text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train = Train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del books\n",
    "del Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_ids, book_ids = Train.columns, Train.index\n",
    "pickle.dump(user_ids,open('user_ids.p','wb'))\n",
    "pickle.dump(book_ids,open('book_ids.p','wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,"
     ]
    }
   ],
   "source": [
    "Rec_System, items = predictions(Train,'Clear_Text.txt',Items, user_ids, book_ids, alfa = 0)  #Create Predicted Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0399135782</th>\n",
       "      <th>0440234743</th>\n",
       "      <th>0452264464</th>\n",
       "      <th>0609804618</th>\n",
       "      <th>1841721522</th>\n",
       "      <th>0971880107</th>\n",
       "      <th>0345402871</th>\n",
       "      <th>0345417623</th>\n",
       "      <th>0375406328</th>\n",
       "      <th>0446310786</th>\n",
       "      <th>...</th>\n",
       "      <th>0446350397</th>\n",
       "      <th>0671743813</th>\n",
       "      <th>3791513265</th>\n",
       "      <th>3423128801</th>\n",
       "      <th>3785548486</th>\n",
       "      <th>3407785682</th>\n",
       "      <th>325723287X</th>\n",
       "      <th>3764501782</th>\n",
       "      <th>3518376055</th>\n",
       "      <th>3803112133</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>5.207295</td>\n",
       "      <td>4.535249</td>\n",
       "      <td>4.560136</td>\n",
       "      <td>4.707113</td>\n",
       "      <td>4.772182</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.700871</td>\n",
       "      <td>4.700871</td>\n",
       "      <td>5.243979</td>\n",
       "      <td>4.772929</td>\n",
       "      <td>...</td>\n",
       "      <td>4.717803</td>\n",
       "      <td>4.669795</td>\n",
       "      <td>4.669262</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.694348</td>\n",
       "      <td>4.721269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>5.256202</td>\n",
       "      <td>4.626278</td>\n",
       "      <td>4.984511</td>\n",
       "      <td>4.636064</td>\n",
       "      <td>4.587660</td>\n",
       "      <td>4.675312</td>\n",
       "      <td>4.628440</td>\n",
       "      <td>4.628440</td>\n",
       "      <td>4.684589</td>\n",
       "      <td>4.698749</td>\n",
       "      <td>...</td>\n",
       "      <td>4.690080</td>\n",
       "      <td>4.721633</td>\n",
       "      <td>4.680955</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.610272</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.681719</td>\n",
       "      <td>4.700093</td>\n",
       "      <td>4.683262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>4.580091</td>\n",
       "      <td>4.650848</td>\n",
       "      <td>4.658290</td>\n",
       "      <td>4.920723</td>\n",
       "      <td>4.652948</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.577532</td>\n",
       "      <td>4.577532</td>\n",
       "      <td>4.894674</td>\n",
       "      <td>4.482750</td>\n",
       "      <td>...</td>\n",
       "      <td>4.673315</td>\n",
       "      <td>4.658204</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.664478</td>\n",
       "      <td>4.673620</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.688285</td>\n",
       "      <td>4.721269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>4.846086</td>\n",
       "      <td>4.967522</td>\n",
       "      <td>4.867693</td>\n",
       "      <td>4.593944</td>\n",
       "      <td>4.744066</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.444073</td>\n",
       "      <td>4.444073</td>\n",
       "      <td>4.798861</td>\n",
       "      <td>4.563464</td>\n",
       "      <td>...</td>\n",
       "      <td>4.741302</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.689157</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.638603</td>\n",
       "      <td>4.721269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.698892</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.727393</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.788925</td>\n",
       "      <td>4.788925</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.409497</td>\n",
       "      <td>...</td>\n",
       "      <td>4.770889</td>\n",
       "      <td>4.804698</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "      <td>4.721269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0399135782  0440234743  0452264464  0609804618  1841721522  0971880107  \\\n",
       "User                                                                           \n",
       "243     5.207295    4.535249    4.560136    4.707113    4.772182    4.721269   \n",
       "254     5.256202    4.626278    4.984511    4.636064    4.587660    4.675312   \n",
       "507     4.580091    4.650848    4.658290    4.920723    4.652948    4.721269   \n",
       "638     4.846086    4.967522    4.867693    4.593944    4.744066    4.721269   \n",
       "643     4.721269    4.721269    4.698892    4.721269    4.727393    4.721269   \n",
       "\n",
       "      0345402871  0345417623  0375406328  0446310786     ...      0446350397  \\\n",
       "User                                                     ...                   \n",
       "243     4.700871    4.700871    5.243979    4.772929     ...        4.717803   \n",
       "254     4.628440    4.628440    4.684589    4.698749     ...        4.690080   \n",
       "507     4.577532    4.577532    4.894674    4.482750     ...        4.673315   \n",
       "638     4.444073    4.444073    4.798861    4.563464     ...        4.741302   \n",
       "643     4.788925    4.788925    4.721269    4.409497     ...        4.770889   \n",
       "\n",
       "      0671743813  3791513265  3423128801  3785548486  3407785682  325723287X  \\\n",
       "User                                                                           \n",
       "243     4.669795    4.669262    4.721269    4.721269    4.721269    4.721269   \n",
       "254     4.721633    4.680955    4.721269    4.721269    4.610272    4.721269   \n",
       "507     4.658204    4.721269    4.721269    4.664478    4.673620    4.721269   \n",
       "638     4.721269    4.721269    4.721269    4.721269    4.689157    4.721269   \n",
       "643     4.804698    4.721269    4.721269    4.721269    4.721269    4.721269   \n",
       "\n",
       "      3764501782  3518376055  3803112133  \n",
       "User                                      \n",
       "243     4.721269    4.694348    4.721269  \n",
       "254     4.681719    4.700093    4.683262  \n",
       "507     4.721269    4.688285    4.721269  \n",
       "638     4.721269    4.638603    4.721269  \n",
       "643     4.721269    4.721269    4.721269  \n",
       "\n",
       "[5 rows x 9364 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the result\n",
    "Rec_System = ((Rec_System - np.min(Rec_System.values))/(np.max(Rec_System.values) - np.min(Rec_System.values)))*10\n",
    "Rec_System.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2492, 2928)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Table that contains users and their corresponding already read books\n",
    "Read_Books = rated_items(Train,(Train.shape[1],Train.notnull().sum().max()),user_ids)\n",
    "Read_Books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create pd.Series of Year from items with integer type\n",
    "Items = Items.Year.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0399135782</th>\n",
       "      <th>0440234743</th>\n",
       "      <th>0452264464</th>\n",
       "      <th>0609804618</th>\n",
       "      <th>1841721522</th>\n",
       "      <th>0971880107</th>\n",
       "      <th>0345402871</th>\n",
       "      <th>0345417623</th>\n",
       "      <th>0375406328</th>\n",
       "      <th>0446310786</th>\n",
       "      <th>...</th>\n",
       "      <th>0446350397</th>\n",
       "      <th>0671743813</th>\n",
       "      <th>3791513265</th>\n",
       "      <th>3423128801</th>\n",
       "      <th>3785548486</th>\n",
       "      <th>3407785682</th>\n",
       "      <th>325723287X</th>\n",
       "      <th>3764501782</th>\n",
       "      <th>3518376055</th>\n",
       "      <th>3803112133</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.087535</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.051821</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.031513</td>\n",
       "      <td>0.067227</td>\n",
       "      <td>0.016106</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.123249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111345</td>\n",
       "      <td>0.087535</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.031513</td>\n",
       "      <td>0.055322</td>\n",
       "      <td>0.055322</td>\n",
       "      <td>0.031513</td>\n",
       "      <td>0.055322</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.055322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.063095</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.027381</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.044048</td>\n",
       "      <td>0.044048</td>\n",
       "      <td>0.098810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086905</td>\n",
       "      <td>0.063095</td>\n",
       "      <td>0.020238</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.079762</td>\n",
       "      <td>0.079762</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.079762</td>\n",
       "      <td>0.044048</td>\n",
       "      <td>0.079762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 9364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0399135782  0440234743  0452264464  0609804618  1841721522  0971880107  \\\n",
       "User                                                                           \n",
       "243     0.087535    0.007703    0.051821    0.007703    0.031513    0.067227   \n",
       "254     0.063095    0.032143    0.027381    0.032143    0.055952    0.091667   \n",
       "\n",
       "      0345402871  0345417623  0375406328  0446310786     ...      0446350397  \\\n",
       "User                                                     ...                   \n",
       "243     0.016106    0.019608    0.019608    0.123249     ...        0.111345   \n",
       "254     0.008333    0.044048    0.044048    0.098810     ...        0.086905   \n",
       "\n",
       "      0671743813  3791513265  3423128801  3785548486  3407785682  325723287X  \\\n",
       "User                                                                           \n",
       "243     0.087535    0.004202    0.031513    0.055322    0.055322    0.031513   \n",
       "254     0.063095    0.020238    0.055952    0.079762    0.079762    0.055952   \n",
       "\n",
       "      3764501782  3518376055  3803112133  \n",
       "User                                      \n",
       "243     0.055322    0.019608    0.055322  \n",
       "254     0.079762    0.044048    0.079762  \n",
       "\n",
       "[2 rows x 9364 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YearDiff = Diff_Matrix(Rec_System,Read_Books,Items) #Create Matrix which contains year diff. between avg year published for user and each item\n",
    "YearDiff.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save table for ISBN to number and vice versa translation\n",
    "itz = pd.Series(range(len(Items)), index = Items.index)\n",
    "itz = pd.DataFrame({'ISBN' : itz.index, 'Key' : itz.values}).set_index('ISBN')\n",
    "pickle.dump(itz,open('Items.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save these three datasets on the HD\n",
    "pickle.dump(itz,open('Items.p','wb'))\n",
    "pickle.dump(Rec_System,open('Rec_System_modif.p','wb'))\n",
    "pickle.dump(Read_Books,open('Read_Books_modif.p','wb'))\n",
    "pickle.dump(YearDiff,open('YearDiff_modif.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Recommended ISBNs for each User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Files\n",
    "a = pickle.load(open('Rec_System_modif.p','rb'))\n",
    "b = pickle.load(open('Read_Books_modif.p','rb'))\n",
    "c = pickle.load(open('YearDiff_modif.p','rb'))\n",
    "d = pickle.load(open('Items.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ISBN_RECOMMENDATIONS = ISBN_REC_SYS(a,b,c,d,50) #Create matrix of Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ISBN_RECOMMENDATIONS = pd.DataFrame(ISBN_RECOMMENDATIONS) #Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9354</th>\n",
       "      <th>9355</th>\n",
       "      <th>9356</th>\n",
       "      <th>9357</th>\n",
       "      <th>9358</th>\n",
       "      <th>9359</th>\n",
       "      <th>9360</th>\n",
       "      <th>9361</th>\n",
       "      <th>9362</th>\n",
       "      <th>9363</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>2794.0</td>\n",
       "      <td>8649.0</td>\n",
       "      <td>3485.0</td>\n",
       "      <td>8249.0</td>\n",
       "      <td>5891.0</td>\n",
       "      <td>6435.0</td>\n",
       "      <td>5894.0</td>\n",
       "      <td>5499.0</td>\n",
       "      <td>5893.0</td>\n",
       "      <td>4346.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2794.0</td>\n",
       "      <td>8649.0</td>\n",
       "      <td>3485.0</td>\n",
       "      <td>8249.0</td>\n",
       "      <td>5891.0</td>\n",
       "      <td>6435.0</td>\n",
       "      <td>5895.0</td>\n",
       "      <td>5894.0</td>\n",
       "      <td>5499.0</td>\n",
       "      <td>5893.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 9364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3       4       5       6       7       8     \\\n",
       "User                                                                           \n",
       "243   2794.0  8649.0  3485.0  8249.0  5891.0  6435.0  5894.0  5499.0  5893.0   \n",
       "254   2794.0  8649.0  3485.0  8249.0  5891.0  6435.0  5895.0  5894.0  5499.0   \n",
       "\n",
       "        9     ...   9354  9355  9356  9357  9358  9359  9360  9361  9362  9363  \n",
       "User          ...                                                               \n",
       "243   4346.0  ...   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  \n",
       "254   5893.0  ...   -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  \n",
       "\n",
       "[2 rows x 9364 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ISBN_RECOMMENDATIONS.index = pickle.load(open('user_ids.p','rb'))     #Assign labels to rows\n",
    "ISBN_RECOMMENDATIONS.head(2) #Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(ISBN_RECOMMENDATIONS, open('CB_RS_ISBNS{}_modif.p'.format(Testing),'wb')) #Save to the HD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create Recommended Ratings for each User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Files\n",
    "# a = pickle.load(open('Rec_System_modif.p','rb'))\n",
    "# b = pickle.load(open('Read_Books_modif.p','rb'))\n",
    "# c = pickle.load(open('YearDiff_modif.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RATINGS_RECOMMENDATIONS = RATINGS_REC_SYS(a,b,c,50) #Create matrix of Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RATINGS_RECOMMENDATIONS = pd.DataFrame(RATINGS_RECOMMENDATIONS) #Create DataFrame\n",
    "RATINGS_RECOMMENDATIONS.index = pickle.load(open('user_ids.p','rb'))     #Assign labels to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Translate to Ratings format\n",
    "RATINGS_RECOMMENDATIONS = ((RATINGS_RECOMMENDATIONS-np.min(RATINGS_RECOMMENDATIONS.values))/\n",
    "                            (np.max(RATINGS_RECOMMENDATIONS.values)-np.min(RATINGS_RECOMMENDATIONS.values)))*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9354</th>\n",
       "      <th>9355</th>\n",
       "      <th>9356</th>\n",
       "      <th>9357</th>\n",
       "      <th>9358</th>\n",
       "      <th>9359</th>\n",
       "      <th>9360</th>\n",
       "      <th>9361</th>\n",
       "      <th>9362</th>\n",
       "      <th>9363</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>9.380069</td>\n",
       "      <td>8.648343</td>\n",
       "      <td>8.328004</td>\n",
       "      <td>5.978153</td>\n",
       "      <td>5.917954</td>\n",
       "      <td>5.871558</td>\n",
       "      <td>5.863437</td>\n",
       "      <td>5.862846</td>\n",
       "      <td>5.862522</td>\n",
       "      <td>5.862226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>9.190616</td>\n",
       "      <td>8.433846</td>\n",
       "      <td>8.113443</td>\n",
       "      <td>5.784368</td>\n",
       "      <td>5.656130</td>\n",
       "      <td>5.636524</td>\n",
       "      <td>5.633754</td>\n",
       "      <td>5.623528</td>\n",
       "      <td>5.621437</td>\n",
       "      <td>5.620293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 9364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "User                                                                         \n",
       "243   9.380069  8.648343  8.328004  5.978153  5.917954  5.871558  5.863437   \n",
       "254   9.190616  8.433846  8.113443  5.784368  5.656130  5.636524  5.633754   \n",
       "\n",
       "          7         8         9     ...   9354  9355  9356  9357  9358  9359  \\\n",
       "User                                ...                                        \n",
       "243   5.862846  5.862522  5.862226  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "254   5.623528  5.621437  5.620293  ...    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "      9360  9361  9362  9363  \n",
       "User                          \n",
       "243    0.0   0.0   0.0   0.0  \n",
       "254    0.0   0.0   0.0   0.0  \n",
       "\n",
       "[2 rows x 9364 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RATINGS_RECOMMENDATIONS.head(2)  #Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(RATINGS_RECOMMENDATIONS, open('CB_RS_SCORES{}_modif.p'.format(Testing),'wb')) #Save to the HD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error = MAE('Test{}cb.p'.format(Testing),'CB_RS_SCORES{}_modif.p'.format(Testing),  #Computer Mean Absolute Error\n",
    "            'CB_RS_ISBNS{}_modif.p'.format(Testing),'Items.p') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25666225071541016"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(error, open('cb_error{}.p'.format(Testing),'wb')) #Save to the HD\n",
    "error/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Error on average is 2.5649\n"
     ]
    }
   ],
   "source": [
    "# Display Cross Validation error of all test sets\n",
    "CV_error = np.zeros(5)\n",
    "for i in range(5):\n",
    "    CV_error[i] = pickle.load(open('cb_error{}.p'.format(i),'rb'))\n",
    "print('Cross Validation Error on average is {}'.format(np.round(np.mean(CV_error),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Online Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items= pickle.load(open('items.p','rb'))\n",
    "Items= pickle.load(open('Itemz.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended books: ['0553572377' '067152609X' '185326086X' '1853260622' '1853260053']\n",
      "Predicted ratings: [ 10.  10.  10.  10.  10.]\n"
     ]
    }
   ],
   "source": [
    "user = pd.read_csv('input_cb.txt',encoding='utf8', header = None, index_col=0, names=['ISBN','Rating']) #Read input\n",
    "user_profile = single_user(user) \n",
    "sim = cosine_similarity(user_profile.reshape(1,-1), items).flatten()\n",
    "Recommend(sim,Items,user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of whole process is 28 minutes.\n"
     ]
    }
   ],
   "source": [
    "print('Time of whole process is {} minutes.'.format(round(time.clock()-t)//60,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
